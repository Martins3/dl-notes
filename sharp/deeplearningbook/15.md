# Deep Generative Models
1. it seems that the chp 16 17 18 19 is for this chapeter ? list what is the fundamental to this chapter !
2. some of this model are structured probabilistic models described in terms of grapg and factors. the question is what is factors ?

## 1 Boltzmann Machines
The Boltzmann machine becomes more powerful when not all the variables are observed
1. Boltzmann machines were originally introduced as a general “connectionist” approach to learning arbitrary probability distributions over binary vectors: why binary vectors ?
2. perform inference in the model : how to implement this ?
3.  why we choose the energy function 20.2
4. it does limit the kinds of interactions between the observed variables to those described by the weight matrix. Specifically, it means that the probability of one unit being on is given by a linear model (logistic regression) from the values of the other units.
5. from 20.2 to 20.3 is not natural at all !
6. One interesting property of Boltzmann machines when trained with learning rules based on maximum likelihood is that the update for a particular weight connecting two units depends only the statistics of those two units: how to prove this
7. The negative phase of Boltzmann machine learning is somewhat harder to explain from a biological point of view: negtive phase of Boltzmann machine and positive phase of Boltzmann machine

## 2 Restricted Boltzmann Machines
RBMs are undirected probabilistic graphical models containing a layer of observable variables and a single layer of latent variables
RBMs may be stacked (one on top of the other) to form deeper models
1. deep belief work :A deep belief network is a hybrid graphical model involving both directed and undirected connections
2. A deep Boltzmann machine is an undirected graphical model with several layers of latent variables
3. It is apparent from the definition of the partition function Z that the naive method of computing Z (exhaustively summing over all states) could be computationally intractable: I can not see it

### 2.1 Conditional Distributions

### 2.2 Training Restricted Boltzmann Machines
1. Because the RBM admits efficient evaluation and differentiation of P ˜(v) and efficient MCMC sampling in the form of block Gibbs sampling, it can readily be trained with any of the techniques described in Chapter 18 for training models that have intractable partition functions

## 3 Deep Belief Networks
Today, deep belief networks have mostly fallen out of favor and are rarely used, even compared to other unsupervised or generative learning algorithms
1. 仙人说唱， give up, if have time review

## 4 Deep Boltzmann Machines
entirely undirected model with several hidden layers
明天早上，感觉不是很难

## 5 Boltzmann Machines for Real-Valued Data
## 6 Convolutional Boltzmann Machines

## 7 Boltzmann Machines for Real-Valued Data
1.

## 8 Other Boltzmann Machines

## 9 Back-Propagation through Random Operations

## 10 Directed Generative Nets
### 10.1 Sigmoid Belief Nets
The most common structure of sigmoid belief network is one that is divided into many layers, with ancestral sampling proceeding through a series of many hidden layers and then ultimately generating the visible layer
1. no example, how can I understand what it express
### 10.2 Differentiable Generator Nets

### 10.3 Variational Autoencoders
The variational autoencoder or VAE is a directed model that uses learned approximate inference and can be trained purely with gradient-based methods

## 11 Drawing Samples from Autoencoders

## 12 Generative Stochastic Networks

## 14 Evaluating Generative Models

## 15 Conclusion
Training generative models with hidden units is a powerful way to make models understand the world represented in the given training data
