# Approximate Inference
Approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy, when exact learning and inference are computationally intractable.
Throughout this chapter, we will show how to derive different forms of approximate inference by using approximate optimization to find q
The challenge of inference usually refers to the difficult problem of computing p(h | v) or taking expectations with respect to it
1. From a probabilistic perspective, we can frame the problem of learning as an inference problem: what is inference here
2. Figure 19.1, why these structure make inference difficult ?
3. https://www.quora.com/What-is-approximate-inference  https://en.wikipedia.org/wiki/Latent_variable
4. what is the relation between hidden variables between latent variables ?

## 1 Inference as Optimization
1. marginalize out h
2. evidence lower bound(negtive variational free energy)

## 2 Expectation Maximization
<!-- 老子又看见你了, this need to review 李航s book  -->
EM is not an approach to approximate inference, but rather an approach to learning with an approximate posterior
1. analyze the EM algorithm, and try on it

## 3 MAP Inference and Sparse Coding
<!-- 需要首先review MAP 算法 -->
We usually use the term inference to refer to computing the probability distribution over one set of variables given another
An alternative form of inference is to compute the single most likely value of the missing variables, rather than to infer the entire distribution over their possible values

## 4 Variational Inference and Learning
