# Optimization for Training deep Models
1. an very important questions seems to be forget, gradient disappearence !
2. what is the  important aspects we need to carefully handle ? effcience , numerical weekness , memory
3. this chapter don't disscuss how to calculate gradient, but the efficiency ?

## 7 Optimization Strategies and Meta-Algorithm
1. adaptive reparametrization :question:
2. make the networks in group and everytime update only one of the group, can this make a anything difference ? for example, this 1 ~ 10 layers as the first group and so on !
3. give an example, that we have more than one solutions for an networks ? for the xor, is there a close form of W  w and b
<!-- 可以测试一个 y = x* w1 * w2 * w3 的网络， 看一看运行的时候的问题是什么 ？ 第一件事情就是 有没有可以使用不同的 learning rate 对于每一层  -->
4.
### Batch Normalization
