# Sequence Modeling: Recurrent and Recursive Nets
1. why tanh not the relu ?
2.

## 2 Recurrent Neural Networks

### 2.2 Computing the Gradient in a Recurrent Neural Network
The use of back-propagation on the unrolled gaph is called back-propagation through time algorithm(BPTT)
1. why formula 10.19 has transposit ?
2. prove the formula 10.21
3. Once the gradient of internal nodes of the computational graph are obtained, we can obtain the gradient on the parameter nodes. I don't think there are order here!

### 2.3 Recurrent Networks as Directed Graphical Models
parameter sharing used in the recurrent networks relies on the assumption that the same parameters can be used for different time steps
one way to view the rnn as defininga graphical model whose structure is the complete graph
It is more interesting to consider the graphical model structure of RNNs that results from regarding the hidden units h as random variables
<!-- the main difficult here is that we dont know what the graphical models are ? -->
1. one step probability prediction means what ?
2. about the figure 10.8, why the parameters numebrs change from O(kt) to O(1)
3. In order to view the RNN as graphical model, what should we do except describing how to draw samples from the model ？
4. really don't understand why the length become the main obstacle of the sampling ?
5. neither of the three ways to get the length can I understand ? epecially the third one!
6. how to understand "no input x":  RNN could correspond to a directed graphical model over a sequence of random variables y with no input x.

### 2.4 Modeling Sequences Conditioned on the Context with RNNs

1. figure 10.9, why this model can be used for the image captioning ?
2. conditional recurrent neural network different with rnn in which way, in another word, what is the means of the conditional here ?
3.

## 3 Bidirectional RNNs

## 4 Encoder-Decoder Sequence-to-Sequence Architectures

## 5 Deep Recurrent Networks

## 6 Recursive Neural Networks
recursive neural network is structured as a deep tree
one clear davantage of recursive net over recurrent net is reducing the depth, might help deal with long-term dependencies
1. processing *data structure* as input to neural nets

## 7 The Challenge of Long-Term dependencies
composition of the same function multiple times can result in extremely nonlinear behavior
in order to store memorise in a way that is robust to small perturbations, the RNN must enter a region of parameter space where gradient vanish
whenever the model is able to represent long term dependencies, the gradient of a long term interacion has much smaller magnitude
1. figure 10.15 what is the x and different lines means what ?
2. increse the span of dependencies. what is the span of dependencies ?

## 8 Echo State Networks
(这一章看不懂，啥j8玩意儿)
set the recurrent weights such that the recurrent hidden units do a good job of capturing the history of past inputs, and only learn the output weigths
1. the problem of interest, this is not first time I come across the term interest >
2. why we should set the recurrent  weigths such that the dynamical system in near the edge of stability

## 9 Leaky Units and Other Strategeies for Multiple Times Scales
