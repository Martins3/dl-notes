### start

1. deep learning can only handle the problem that transfer one vector into another vector
2. convolutional network => scaling to high resolutional image   
    recurent network => temporal seqence,can be used for NLP(natural language processing)


# deep feedforward network
## review
1. Deep feedforward networks, also often called feedforward neural networks, or multi-
layer perceptrons (MLPs), are the quintessential(精髓) deep learning models.
2. These models are called feedforward because information flows through the
funciton being evaluated from x, through the intermediate computations used to
define f , and finally to the output y.
3. when MLPs are extended to include feedback connections,it's called rnn
4. unit-> vector to scalar function
5. Linear models, such as logistic regression and **linear regression**, are appealing because they may be fit efficiently and reliably, either in **closed form** or with convex optimization. Linear models also
have the obvious defect that the model capacity is limited to linear functions, so
**the model cannot understand the interaction between any two input variables**.
> [rbf kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) 径向基函数  

6. To extend linear models to represent nonlinear functions of x, we can apply
the linear model not to x itself but to a transformed input φ(x), where φ is a nonlinear transformation
7. how to choose the φ
  1.  manually engineer φ, this need the masters to handle
  2.  a generic φ, for example kernel machines  
  3. deep learning to learn the  φ
8. design decisions needed to deploy a feedforward network
  1.  choosing the optimizer
  2. the cost functions
  3. the form of output unit
  4.  activation functions for hidden layer
  5. how many layer :apple:  how many units in the each layer :apple: how they connected

## Example: learning XOR
1. nonlinear part the activation funciton, mordern neural network recommanded the ReUL

##  Gradient-Based Learning
1. nonlinearity of a neural network causes most interesting loss
functions to become non-convex. :confused: *ReUL is not convex ?*
2. non-convex using iterative, gradient-based optimizers that merely drive the costfunction to a very low
value, rather than the linear equation solvers used to train linear regression models or the  
convex optimization algorithms with global convergence guarantees used to train logistic regression or  
SVMs. :confused: *so many conclusion , but i don't know why*
3. Stochastic gradient descent applied to non-convex loss functions has no such convergence guarantee, and is sensitive to the values of the initial parameters. For feedforward neural networks, initialized to zero or to small positive values. :confused: *why bais should be 0 or 0.0001 ?*
4. Cost Functions
  1. Learning Conditional Distributions with Maximum Likelihood
    1. Maximum Likelihood means what ? :smile: *if you forget, please go fuck yourself*
    2. equivalently described as  cross-entropy
    3. advantage: not saturate
    4. Regularization techniques will the discussed later to handle the disadvantage
  2. Learning Conditional Statistics
    1. Instead of learning a full probability distribution p(y | x ; θ) we often want to learn
just one conditional statistic of y given x.
    2. for example the mean of y, we use the expression [](6.14)
5. output units
  1. Linear Units for Gaussian Output Distributions
  2. sogmid for Bernoulli Output Distributions
  3. Softmax Units for Multinoulli Output Distributions
  4. other format
    1. [precision matrix](https://www.statlect.com/glossary/precision-matrix)the eigenvalues of the
    precision matrix are the reciprocals of the eigenvalues of the covariance matrix
    2. :sob: *really don't know what it's talking about*
6. Hidden Units
  1. The design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles.







---
1. For discrete output variables, most models are parametrized in such a way that they cannot represent a
probability of zero or one, but can come arbitrarily close to doing so. Logistic regressionis an example
of such a model. For real-valued output variables, if the model can control the density of the output
distribution (for example, by learning the variance parameter of a Gaussian output distribution) then it
becomes possible to assign extremely high density to the correct training set outputs, resulting in
cross-entropy approaching negative infinity.
