# Regularization
the model is trained such that it does not learn interdependent set of features weights
# overall
1. why we need regularization ?
2. what is the main ideas for the regularization ? do we have any assumputions for this ?
3. what is maniflod ? definition, example , and connections to other fields
4.


## 1 parameter norm penalties
1. why we should make the W close to the zero
2. sparsity has good properties in which aspects ? there are more than circumstance I see this terminology ?
3. caluculate the formulator by code

## 2 norm penalties as constrained optimization
should read section 4.4 and know more things about the CO :confused:
## 3 Regularization and under-constrained problem
necessary for machining learning problem to be properly define
## 4 Data Argumentation


generate more data from the real dataset
## 5 Noise Robustness
add noise on weight or output
## 6 semi-supervised learning

## 7 Mutil-task Learning

## 8 Early Stopping

## 9 Parameter Tying and Parameter Sharing

## 10 Sparse Representation


## 11 Bagging and other Ensemble Methods

## 12 Dropout
<!-- 可以控制整个网络的复杂程度 -->
dropout is a form of regularization (it constrains network adaptation to the data at training time, to avoid it becoming “too smart” in learning the input data; it thus helps to avoid overfitting
1. why keep probability is different in the training and testing ?

## 13 Adversarial Training
1. import a great idea and an phenomenon where invariant to local changes ?

## 14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier\
1. what are the ways of overcome the dimensionality ? list as much as possible.
2.


> key words : low-dimension maniflod tangent

> :confused: non-parametric


```
1. https://en.wikipedia.org/wiki/Regularization_(mathematics)
2.
